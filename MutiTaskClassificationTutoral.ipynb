{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled31.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN76kqw803s0SWNUZeyNWZp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chengyang122/mutitaskNAM/blob/main/MutiTaskClassificationTutoral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/chengyang122/mutitaskNAM.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUDChpQzl9Ro",
        "outputId": "bb7096e3-0fe6-4b02-d4c1-fc6dc42cb745"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mutitaskNAM'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 24 (delta 2), reused 21 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd mutitaskNAM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRytGJClmJYC",
        "outputId": "a849da02-002d-4a92-86d6-e7a41e6e4518"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mutitaskNAM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tqdm\n",
        "import copy\n",
        "import random\n",
        "import logging\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pandas as pd\n",
        "import nam.metrics\n",
        "import nam.data_utils\n",
        "from nam.model import *"
      ],
      "metadata": {
        "id": "BVdrhJXKwJL1"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union, Iterable, Sized, Tuple\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def truncated_normal_(tensor, mean: float = 0., std: float = 1.):\n",
        "    size = tensor.shape\n",
        "    tmp = tensor.new_empty(size + (4,)).normal_()\n",
        "    valid = (tmp < 2) & (tmp > -2)\n",
        "    ind = valid.max(-1, keepdim=True)[1]\n",
        "    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
        "    tensor.data.mul_(std).add_(mean)\n",
        "\n",
        "\n",
        "class ActivationLayer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features: int,\n",
        "                 out_features: int):\n",
        "        super().__init__()\n",
        "        self.weight = torch.nn.Parameter(torch.empty((in_features, out_features)))\n",
        "        self.bias = torch.nn.Parameter(torch.empty(in_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError(\"abstract method called\")\n",
        "\n",
        "\n",
        "class ExULayer(ActivationLayer):\n",
        "    def __init__(self,\n",
        "                 in_features: int,\n",
        "                 out_features: int):\n",
        "        super().__init__(in_features, out_features)\n",
        "        truncated_normal_(self.weight, mean=4.0, std=0.5)\n",
        "        truncated_normal_(self.bias, std=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        exu = (x - self.bias) @ torch.exp(self.weight)\n",
        "        return torch.clip(exu, 0, 1)\n",
        "\n",
        "\n",
        "class ReLULayer(ActivationLayer):\n",
        "    def __init__(self,\n",
        "                 in_features: int,\n",
        "                 out_features: int):\n",
        "        super().__init__(in_features, out_features)\n",
        "        torch.nn.init.xavier_uniform_(self.weight)\n",
        "        truncated_normal_(self.bias, std=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu((x - self.bias) @ self.weight)\n",
        "\n",
        "\n",
        "class FeatureNN(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 shallow_units: int,\n",
        "                 hidden_units: Tuple = (),\n",
        "                 shallow_layer: ActivationLayer = ExULayer,\n",
        "                 hidden_layer: ActivationLayer = ReLULayer,\n",
        "                 dropout: float = .5,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.ModuleList([\n",
        "            hidden_layer(shallow_units if i == 0 else hidden_units[i - 1], hidden_units[i])\n",
        "            for i in range(len(hidden_units))\n",
        "        ])\n",
        "        self.layers.insert(0, shallow_layer(1, shallow_units))\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.linear = torch.nn.Linear(shallow_units if len(hidden_units) == 0 else hidden_units[-1], 1, bias=False)\n",
        "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "            x = self.dropout(x)\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "class NeuralAdditiveModel(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 shallow_units: int,\n",
        "                 hidden_units: Tuple = (),\n",
        "                 shallow_layer: ActivationLayer = ExULayer,\n",
        "                 hidden_layer: ActivationLayer = ReLULayer,\n",
        "                 feature_dropout: float = 0.,\n",
        "                 hidden_dropout: float = 0.,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "\n",
        "        if isinstance(shallow_units, list):\n",
        "            assert len(shallow_units) == input_size\n",
        "        elif isinstance(shallow_units, int):\n",
        "            shallow_units = [shallow_units for _ in range(input_size)]\n",
        "\n",
        "        self.feature_nns = torch.nn.ModuleList([\n",
        "            FeatureNN(shallow_units=shallow_units[i],\n",
        "                      hidden_units=hidden_units,\n",
        "                      shallow_layer=shallow_layer,\n",
        "                      hidden_layer=hidden_layer,\n",
        "                      dropout=hidden_dropout)\n",
        "            for i in range(input_size)\n",
        "        ])\n",
        "        self.feature_dropout = torch.nn.Dropout(p=feature_dropout)\n",
        "        self.bias = torch.nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        f_out = torch.cat(self._feature_nns(x), dim=-1)\n",
        "        f_out = self.feature_dropout(f_out)\n",
        "\n",
        "        return f_out.sum(axis=-1) + self.bias, f_out\n",
        "\n",
        "    def _feature_nns(self, x):\n",
        "        return [self.feature_nns[i](x[:, i]) for i in range(self.input_size)]\n"
      ],
      "metadata": {
        "id": "PxCJ-XKbyZ6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changed the FeatureNN and NeuralAdditiveFunction\n",
        "class FeatureNN(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 shallow_units: int,\n",
        "                 hidden_units: Tuple = (),\n",
        "                 shallow_layer: ActivationLayer = ExULayer,\n",
        "                 hidden_layer: ActivationLayer = ReLULayer,\n",
        "                 dropout: float = .5,\n",
        "                 output_size = 1\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.ModuleList([\n",
        "            hidden_layer(shallow_units if i == 0 else hidden_units[i - 1], hidden_units[i])\n",
        "            for i in range(len(hidden_units))\n",
        "        ])\n",
        "        self.layers.insert(0, shallow_layer(1, shallow_units))\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.linear = torch.nn.Linear(shallow_units if len(hidden_units) == 0 else hidden_units[-1], output_size, bias=False)\n",
        "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "            x = self.dropout(x)\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "class NeuralAdditiveModel(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 output_size: int, #not one when it is muti class, in sigle class regression and classification it would be 1\n",
        "                 shallow_units: int,\n",
        "                 hidden_units: Tuple = (),\n",
        "                 shallow_layer: ActivationLayer = ExULayer,\n",
        "                 hidden_layer: ActivationLayer = ReLULayer,\n",
        "                 feature_dropout: float = 0.,\n",
        "                 hidden_dropout: float = 0.,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "\n",
        "        if isinstance(shallow_units, list):\n",
        "            assert len(shallow_units) == input_size\n",
        "        elif isinstance(shallow_units, int):\n",
        "            shallow_units = [shallow_units for _ in range(input_size)]\n",
        "\n",
        "        self.feature_nns = torch.nn.ModuleList([\n",
        "            FeatureNN(shallow_units=shallow_units[i],\n",
        "                      hidden_units=hidden_units,\n",
        "                      shallow_layer=shallow_layer,\n",
        "                      hidden_layer=hidden_layer,\n",
        "                      dropout=hidden_dropout,\n",
        "                      output_size=output_size)\n",
        "            for i in range(input_size)\n",
        "        ])\n",
        "        self.feature_dropout = torch.nn.Dropout(p=feature_dropout)\n",
        "        self.bias = torch.nn.Parameter(torch.zeros(1))\n",
        "        self.output_size = output_size\n",
        "    def _feature_nns(self, x):\n",
        "        return [self.feature_nns[i](x[:, i]) for i in range(self.input_size)]\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.output_size == 1:\n",
        "          f_out = torch.cat(self._feature_nns(x), dim=-1)\n",
        "        else: \n",
        "          f_out = torch.stack(self._feature_nns(x), dim = -1)\n",
        "        # f_out = self.feature_dropout(f_out)\n",
        "        return f_out.sum(axis=-1) + self.bias, f_out\n",
        "\n"
      ],
      "metadata": {
        "id": "TtroMLM4yfZS"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, (x_test, y_test) = nam.data_utils.create_test_train_fold(dataset='BreastCancer',\n",
        "                                                            id_fold=1,\n",
        "                                                            n_folds=10,\n",
        "                                                            n_splits=3,\n",
        "                                                            regression=False)\n",
        "(x_train, y_train), (x_validate, y_validate) = next(train)\n",
        "x_train.shape\n",
        "y_train.shape\n",
        "len(nam.data_utils.calculate_n_units(x_train, 1000, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_qgybOApmx9",
        "outputId": "d8bc9db8-210f-47c8-f36c-9a2302e92852"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str_']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str_']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = pd.read_csv('data1.csv', index_col=0)\n",
        "target = pd.read_csv('data2.csv', index_col=0)\n",
        "s = target['0']\n",
        "oneHotTarget = pd.get_dummies(s)\n",
        "x_train_new = features.to_numpy()\n",
        "y_train_new = oneHotTarget.to_numpy()"
      ],
      "metadata": {
        "id": "CPYTMBdUAwHF"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralAdditiveModel(\n",
        "    input_size=x_train_new.shape[-1],# feature size, 0 is sample and 1 is the feature, this is one iter of torch dataloader \n",
        "    output_size = y_train_new.shape[-1],\n",
        "    shallow_units=nam.data_utils.calculate_n_units(x_train_new, 1000, 2),#for feature network, it is changing with data and I am not sure why\n",
        "    hidden_units=list(map(int, [])),#for feature network\n",
        "    shallow_layer=ExULayer,#special operational layer designed for this model\n",
        "    hidden_layer=ExULayer,\n",
        "    hidden_dropout=0.3,\n",
        "    feature_dropout=0.0\n",
        "    ).to(device)"
      ],
      "metadata": {
        "id": "PDyCmYA4pifX"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(torch.tensor(x_train_new), torch.tensor(y_train_new))\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "id": "tLYc5efEsssm"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#first output is the sum of all output based on input samples \n",
        "#second output is the contribution of each features of all samples\n",
        "for iter, (feature, target) in enumerate(train_loader):\n",
        "  output, featureoutput = model(feature)\n",
        "  break"
      ],
      "metadata": {
        "id": "4jLm96ZzsPt4"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featureoutput[0].shape #new version, every output class ( output in this case) has (input feature), sum of them will become the results."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKSzl7WWylbW",
        "outputId": "873861e1-0445-4e83-e9db-05936d0a6e0b"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([200, 112])"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = pd.read_csv('data1.csv', index_col=0)"
      ],
      "metadata": {
        "id": "0BILmtrY338d"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_loss(fnn_out, lambda_=0.):\n",
        "    return lambda_ * (fnn_out ** 2).sum() / fnn_out.shape[1]\n",
        "\n",
        "def penalized_cross_entropy(logits, truth, fnn_out, feature_penalty=0.):\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "    return loss(logits, truth.argmax(-1)) + feature_loss(fnn_out, feature_penalty)"
      ],
      "metadata": {
        "id": "yX8IiQdzEGKF"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression = False\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr=1e-3,\n",
        "                              weight_decay=0.0)\n",
        "criterion = nam.metrics.penalized_mse if regression else penalized_cross_entropy\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.995, step_size=1)\n"
      ],
      "metadata": {
        "id": "eTKoIgVFDVpN"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, criterion, optimizer, data_loader, device):\n",
        "    pbar = tqdm.tqdm(enumerate(data_loader, start=1), total=len(data_loader))\n",
        "    total_loss = 0\n",
        "    for i, (x, y) in pbar:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits, fnns_out = model.forward(x)\n",
        "        loss = criterion(logits, y, fnns_out, feature_penalty=0.0)\n",
        "        total_loss -= (total_loss / i) - (loss.item() / i)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(f\"train | loss = {total_loss:.5f}\")\n",
        "    return total_loss\n",
        "\n",
        "train_one_epoch(model, criterion, optimizer, train_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8urjSvNE7Aq",
        "outputId": "f9eed704-76ca-4d67-82a1-72e937b156ec"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train | loss = 1.91559: 100%|██████████| 480/480 [00:32<00:00, 14.96it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.915589958367249"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3W53If3XGrz",
        "outputId": "eb3e1eb0-d93c-44ba-8330-aea4223aa6c9"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 200])"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metric(logits,\n",
        "                     truths,\n",
        "                     regression=True):\n",
        "    \"\"\"Calculates the evaluation metric.\"\"\"\n",
        "    if regression:\n",
        "        # root mean squared error\n",
        "        # return torch.sqrt(F.mse_loss(logits, truths, reduction=\"none\")).mean().item()\n",
        "        # mean absolute error\n",
        "        return \"MAE\", ((logits.view(-1) - truths.view(-1)).abs().sum() / logits.numel()).item()\n",
        "    elif logits.shape[-1] == 1:\n",
        "        # return sklearn.metrics.roc_auc_score(truths.view(-1).tolist(), torch.sigmoid(logits.view(-1)).tolist())\n",
        "        return \"accuracy\", accuracySingle(logits, truths)\n",
        "    else:\n",
        "        return \"accuracy\", accuracyMuti(logits, truths)\n",
        "\n",
        "\n",
        "def accuracyMuti(logits, truths):\n",
        "    return ((logits.argmax(-1)==truths.argmax(-1)).sum()/truths.numel()).item()\n",
        "\n",
        "def accuracySingle(logits, truths):\n",
        "    return (((truths.view(-1) > 0) == (logits.view(-1) > 0.5)).sum() / truths.numel()).item()"
      ],
      "metadata": {
        "id": "DWEqw9r3TYMJ"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    total_score = 0\n",
        "    metric = None\n",
        "    for i, (x, y) in enumerate(data_loader, start=1):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits, fnns_out = model.forward(x)\n",
        "        metric, score = calculate_metric(logits, y, regression=False)\n",
        "        total_score -= (total_score / i) - (score / i)\n",
        "    return metric, total_score"
      ],
      "metadata": {
        "id": "6rLh5BukR1Dy"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, train_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4_yUtPOTuvS",
        "outputId": "cc56a894-eea2-4c43-d48b-dbbbd6b1948c"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('accuracy', 0.0039010417017076816)"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    }
  ]
}