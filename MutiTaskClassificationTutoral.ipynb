{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MutiTaskClassificationTutoral.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0B8/zG/pSDlDBV5CO7Ars",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chengyang122/mutitaskNAM/blob/main/MutiTaskClassificationTutoral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/chengyang122/mutitaskNAM.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUDChpQzl9Ro",
        "outputId": "0735d50e-dec9-4f92-939d-3e66c7a48f9e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'mutitaskNAM' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd mutitaskNAM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRytGJClmJYC",
        "outputId": "194a9928-56d8-4769-cfca-df02c9f37800"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mutitaskNAM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tqdm\n",
        "import copy\n",
        "import random\n",
        "import logging\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pandas as pd\n",
        "from nam.metrics import *\n",
        "import nam.data_utils\n",
        "from nam.model import *"
      ],
      "metadata": {
        "id": "BVdrhJXKwJL1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for muti task classification 200 classes \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "features = pd.read_csv('features.csv', index_col=0)\n",
        "target = pd.read_csv('target.csv', index_col=0, dtype = float)\n",
        "s = target['0']\n",
        "oneHotTarget = pd.get_dummies(s)\n",
        "x_train = features.to_numpy()[:4000]\n",
        "y_train = oneHotTarget.to_numpy()[:4000]\n",
        "x_val = features.to_numpy()[4000:]\n",
        "y_val = oneHotTarget.to_numpy()[4000:]\n",
        "train_dataset = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_dataset = TensorDataset(torch.tensor(x_val), torch.tensor(y_val))\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "criterion = penalized_cross_entropy_MutiTask"
      ],
      "metadata": {
        "id": "CPYTMBdUAwHF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for 2 class classification \n",
        "train, (x_test, y_test) = nam.data_utils.create_test_train_fold(dataset='BreastCancer',\n",
        "                                                                id_fold=1,\n",
        "                                                                n_folds=5,\n",
        "                                                                n_splits=3,\n",
        "                                                                regression=False)\n",
        "(x_train, y_train), (x_validate, y_validate) = next(train)\n",
        "train_dataset = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_dataset = TensorDataset(torch.tensor(x_validate), torch.tensor(y_validate))\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "(x_train, y_train), (x_validate, y_validate) = next(train)\n",
        "\n",
        "criterion = penalized_cross_entropy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTUpdvTV1R8U",
        "outputId": "066882b7-e7ff-4fdc-f3fc-9d25221ce02e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str_']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['str_']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralAdditiveModel(\n",
        "        input_size=x_train.shape[-1],\n",
        "        # feature size, 0 is sample and 1 is the feature, this is one iter of torch dataloader\n",
        "        output_size=1 if len(y_train.shape)==1 else y_train.shape[-1],\n",
        "        shallow_units=nam.data_utils.calculate_n_units(x_train, 1000, 2),\n",
        "        # for feature network, it is changing with data and I am not sure why\n",
        "        hidden_units=list(map(int, [])),  # for feature network\n",
        "        shallow_layer=ExULayer,  # special operational layer designed for this model\n",
        "        hidden_layer=ExULayer,\n",
        "        hidden_dropout=0.3,\n",
        "        feature_dropout=0.0).to(device)"
      ],
      "metadata": {
        "id": "PDyCmYA4pifX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression = False\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr=1e-3,\n",
        "                              weight_decay=0.0)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.995, step_size=1)\n"
      ],
      "metadata": {
        "id": "eTKoIgVFDVpN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, criterion, optimizer, data_loader, device):\n",
        "    pbar = tqdm.tqdm(enumerate(data_loader, start=1), total=len(data_loader))\n",
        "    total_loss = 0\n",
        "    for i, (x, y) in pbar:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits, fnns_out = model.forward(x)\n",
        "        loss = criterion(logits, y, fnns_out, feature_penalty=0.0)\n",
        "        total_loss -= (total_loss / i) - (loss.item() / i)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(f\"train | loss = {total_loss:.5f}\")\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "A8urjSvNE7Aq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    total_score = 0\n",
        "    metric = None\n",
        "    for i, (x, y) in enumerate(data_loader, start=1):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits, fnns_out = model.forward(x)\n",
        "        metric, score = calculate_metric(logits, y, regression=False)\n",
        "        total_score -= (total_score / i) - (score / i)\n",
        "    return metric, total_score"
      ],
      "metadata": {
        "id": "6rLh5BukR1Dy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_scores = []\n",
        "best_validation_score, best_weights = 0, None\n",
        "n_tries = 60\n",
        "for epoch in range(10):\n",
        "    model = model.train()\n",
        "    total_loss = train_one_epoch(model, criterion, optimizer, train_loader, device)\n",
        "    logging.info(f\"epoch {epoch} | train | {total_loss}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    model = model.eval()\n",
        "    metric, val_score = evaluate(model, val_loader, device)\n",
        "    metric, train_score = evaluate(model, train_loader, device)\n",
        "    print(f\"epoch {epoch} | validate | {metric}={val_score}\")\n",
        "    print(f\"epoch {epoch} | train | {metric}={train_score}\")\n",
        "    # early stopping\n",
        "    if val_score <= best_validation_score and n_tries > 0:\n",
        "        n_tries -= 1\n",
        "        continue\n",
        "    elif val_score <= best_validation_score:\n",
        "        logging.info(f\"early stopping at epoch {epoch}\")\n",
        "        break\n",
        "    best_validation_score = val_score\n",
        "    best_weights = copy.deepcopy(model.state_dict())\n",
        "    val_scores.append(val_score)\n",
        "model.load_state_dict(best_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4_yUtPOTuvS",
        "outputId": "c7b982d8-fb66-4978-f673-85be58ad1a58"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train | loss = 5.19127: 100%|██████████| 63/63 [00:05<00:00, 11.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 | validate | accuracy=0.002097355703321787\n",
            "epoch 0 | train | accuracy=0.0024231150321337206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train | loss = 3.11626: 100%|██████████| 63/63 [00:05<00:00, 11.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 | validate | accuracy=0.004162087821616577\n",
            "epoch 1 | train | accuracy=0.0043328371941156315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train | loss = 2.02093: 100%|██████████| 63/63 [00:05<00:00, 11.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 | validate | accuracy=0.004593921610369132\n",
            "epoch 2 | train | accuracy=0.004672618967200083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train | loss = 1.35200: 100%|██████████| 63/63 [00:05<00:00, 11.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 | validate | accuracy=0.004757898250738015\n",
            "epoch 3 | train | accuracy=0.004781745972910098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train | loss = 0.95549: 100%|██████████| 63/63 [00:05<00:00, 11.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4 | validate | accuracy=0.004867788475866502\n",
            "epoch 4 | train | accuracy=0.004877232126004639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train | loss = 0.75463: 100%|██████████| 63/63 [00:05<00:00, 11.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5 | validate | accuracy=0.004723557629264319\n",
            "epoch 5 | train | accuracy=0.004836309479460827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train | loss = 0.58677: 100%|██████████| 63/63 [00:05<00:00, 11.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6 | validate | accuracy=0.004909855707620199\n",
            "epoch 6 | train | accuracy=0.004875992033039292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train | loss = 0.48229: 100%|██████████| 63/63 [00:05<00:00, 11.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7 | validate | accuracy=0.004830013674039107\n",
            "epoch 7 | train | accuracy=0.004875992047822191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train | loss = 0.42841: 100%|██████████| 63/63 [00:05<00:00, 11.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8 | validate | accuracy=0.004830013674039108\n",
            "epoch 8 | train | accuracy=0.004858630916310683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train | loss = 0.36147: 100%|██████████| 63/63 [00:05<00:00, 11.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9 | validate | accuracy=0.004915865353093698\n",
            "epoch 9 | train | accuracy=0.004894593227950354\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}